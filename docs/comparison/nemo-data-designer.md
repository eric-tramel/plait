# plait vs NeMo Data Designer

This document provides a comprehensive comparison between plait and
[NeMo Data Designer](https://nvidia-nemo.github.io/DataDesigner/latest/),
helping you understand when each framework is the right choice and how they
can work together.

## Framework Overview

### NeMo Data Designer

NeMo Data Designer is NVIDIA's framework for generating high-quality synthetic
training data at scale. It focuses on:

- **Schema-based generation**: Define data structure with column types and constraints
- **LLM-powered synthesis**: Use language models to generate realistic data
- **Validation pipelines**: Built-in quality checks and filtering
- **Scalable batch processing**: Generate large volumes efficiently
- **Diversity control**: Manage variety and correlation in generated data

### plait

plait is a PyTorch-inspired framework for LLM inference pipelines, focusing on:

- **Module composition**: Build complex pipelines from reusable components
- **Automatic DAG capture**: Write normal Python code, get optimized execution
- **LLM-based optimization**: Improve pipelines through backward pass feedback
- **Async-first execution**: Maximum throughput with adaptive backpressure

These frameworks serve **fundamentally different purposes** and are often
**complementary** rather than competitive.

## Purpose Comparison

| Aspect | NeMo Data Designer | plait |
|--------|-------------------|-------|
| Primary purpose | Generate synthetic data | Optimize inference pipelines |
| Input | Schema definition | User queries |
| Output | Training datasets | Inference results |
| Optimization target | Data quality/diversity | Pipeline performance |
| Use case timing | Before training | During/after deployment |

## Workflow Implementation Comparison

### Generating Training Data

**NeMo Data Designer approach** - Schema-based generation:

```python
from nemo_data_designer import DataDesigner, Schema, Column
from nemo_data_designer.columns import TextColumn, CategoryColumn, NumericColumn

# Define schema
schema = Schema(
    columns=[
        TextColumn(
            name="question",
            description="Customer support question",
            constraints={"min_length": 10, "max_length": 200},
        ),
        CategoryColumn(
            name="category",
            values=["billing", "technical", "shipping", "returns"],
        ),
        TextColumn(
            name="answer",
            description="Helpful support response",
            constraints={"min_length": 50},
        ),
        NumericColumn(
            name="difficulty",
            min_value=1,
            max_value=5,
        ),
    ]
)

# Configure generator
designer = DataDesigner(
    schema=schema,
    model="gpt-4o",
    diversity_strategy="maximize_coverage",
)

# Generate dataset
dataset = designer.generate(
    num_samples=10000,
    batch_size=100,
    validate=True,
)

# Save for training
dataset.save("customer_support_training.jsonl")
```

**plait approach** - Not designed for bulk data generation:

```python
# plait is for inference, not data generation
# However, you can use the generated data to train plait modules

from plait import Module, LLMInference, Parameter
from plait.optimization import SFAOptimizer, LLMRubricLoss

class CustomerSupport(Module):
    def __init__(self):
        super().__init__()
        self.instructions = Parameter(
            value="You are a helpful customer support agent.",
            description="Agent behavior instructions.",
        )
        self.llm = LLMInference(alias="support", system_prompt=self.instructions)

    def forward(self, question: str) -> str:
        return self.llm(question)

# Load data generated by NeMo Data Designer
with open("customer_support_training.jsonl") as f:
    training_data = [json.loads(line) for line in f]

# Use generated data to optimize the pipeline
module = CustomerSupport().bind(resources=config)
optimizer = SFAOptimizer(module.parameters())

module.train()
for example in training_data:
    output = await module(example["question"])
    feedback = await loss_fn(output, target=example["answer"])
    await feedback.backward()

await optimizer.step()
```

### Processing User Queries

**NeMo Data Designer** - Not designed for live inference:

```python
# Data Designer generates static datasets
# It doesn't process live user queries
```

**plait approach** - Built for inference:

```python
class SupportPipeline(Module):
    def __init__(self):
        super().__init__()
        self.classifier = LLMInference(
            alias="fast",
            system_prompt="Classify the query category.",
        )
        self.responder = LLMInference(
            alias="smart",
            system_prompt="Provide a helpful response.",
        )

    def forward(self, query: str) -> str:
        category = self.classifier(query)
        response = self.responder(f"Category: {category}\nQuery: {query}")
        return response

# Process live requests
pipeline = SupportPipeline().bind(resources=config)
response = await pipeline(user_query)
```

## Key Differentiators

### Data Generation vs Inference Optimization

**NeMo Data Designer** focuses on creating training data:

```python
# Define what data should look like
schema = Schema(
    columns=[
        TextColumn(name="prompt", description="User prompt"),
        TextColumn(name="completion", description="Ideal completion"),
    ]
)

# Generate many examples
dataset = designer.generate(num_samples=50000)

# Use for fine-tuning or evaluation
```

**plait** focuses on optimizing how models are used:

```python
# Define how to process inputs
class Pipeline(Module):
    def forward(self, prompt: str) -> str:
        return self.llm(prompt)

# Optimize based on feedback
await feedback.backward()
await optimizer.step()
```

### Batch Processing vs Async Execution

**NeMo Data Designer** - Optimized for large batch generation:

```python
# Generate in batches
dataset = designer.generate(
    num_samples=100000,
    batch_size=500,
    num_workers=8,
)
```

**plait** - Optimized for concurrent inference:

```python
# Process requests with backpressure
pipeline = Pipeline().bind(resources=config, max_concurrent=100)
results = await pipeline(batch_of_requests)  # Adaptive rate limiting
```

### Validation Approaches

**NeMo Data Designer** - Schema-based validation:

```python
from nemo_data_designer.validators import (
    LengthValidator,
    FormatValidator,
    LLMValidator,
)

schema = Schema(
    columns=[...],
    validators=[
        LengthValidator(column="answer", min_length=50),
        FormatValidator(column="email", pattern=r"^[\w.-]+@[\w.-]+\.\w+$"),
        LLMValidator(
            column="answer",
            criteria="Is this response helpful and accurate?",
        ),
    ]
)
```

**plait** - Loss function based validation:

```python
from plait.optimization import VerifierLoss, LLMRubricLoss, CompositeLoss

loss = CompositeLoss([
    (VerifierLoss(verifier=lambda x: (len(x) > 50, "Too short")), 0.3),
    (LLMRubricLoss(criteria="helpfulness"), 0.7),
])

feedback = await loss(output, target=expected)
```

### Diversity and Correlation

**NeMo Data Designer** has built-in diversity controls:

```python
designer = DataDesigner(
    schema=schema,
    diversity_strategy="maximize_coverage",
    correlation_rules=[
        ("category", "difficulty", 0.3),  # Weak correlation
    ],
)
```

**plait** handles diversity through DAG structure:

```python
class DiverseAnalysis(Module):
    def forward(self, text: str) -> dict:
        # Parallel perspectives provide diversity
        return {
            "technical": self.technical(text),
            "business": self.business(text),
            "user": self.user(text),
        }
```

## Complementary Usage

These frameworks work well together in a typical ML workflow:

### Workflow: Generate Data, Then Optimize Pipeline

```
NeMo Data Designer          plait
       |                      |
       v                      |
  Generate Training      <--- |
      Data                    |
       |                      |
       v                      |
  customer_support.jsonl ---> |
                              v
                        Load Training
                            Data
                              |
                              v
                         Define Module
                              |
                              v
                         Train/Optimize
                              |
                              v
                         Deploy Pipeline
```

### Example Integration

**Step 1: Generate training data with NeMo Data Designer**

```python
from nemo_data_designer import DataDesigner, Schema, Column

schema = Schema(
    columns=[
        Column(name="query", type="text", description="Customer query"),
        Column(name="category", type="category", values=["billing", "tech", "other"]),
        Column(name="ideal_response", type="text", description="Best response"),
        Column(name="quality_score", type="numeric", min=1, max=5),
    ]
)

designer = DataDesigner(schema=schema, model="gpt-4o")
dataset = designer.generate(num_samples=5000, validate=True)
dataset.save("support_training.jsonl")
```

**Step 2: Optimize inference pipeline with plait**

```python
import json
from plait import Module, LLMInference, Parameter
from plait.optimization import SFAOptimizer, LLMRubricLoss, RubricLevel

# Load Data Designer output
with open("support_training.jsonl") as f:
    training_data = [json.loads(line) for line in f]

# Define optimizable pipeline
class SupportAgent(Module):
    def __init__(self):
        super().__init__()
        self.persona = Parameter(
            value="You are a helpful customer support agent.",
            description="Agent personality and behavior guidelines.",
        )
        self.response_style = Parameter(
            value="Be concise and empathetic.",
            description="Response formatting and tone instructions.",
        )
        self.llm = LLMInference(alias="support", system_prompt=self.persona)

    def forward(self, query: str) -> str:
        prompt = f"{self.response_style}\n\nCustomer: {query}"
        return self.llm(prompt)

# Configure optimization
module = SupportAgent().bind(resources=config)
loss_fn = LLMRubricLoss(
    criteria="response quality",
    rubric=[
        RubricLevel(1, "Poor", "Unhelpful or incorrect"),
        RubricLevel(3, "Good", "Helpful and mostly accurate"),
        RubricLevel(5, "Excellent", "Empathetic, accurate, actionable"),
    ],
)
optimizer = SFAOptimizer(module.parameters())

# Train using generated data
module.train()
for example in training_data:
    output = await module(example["query"])
    feedback = await loss_fn(output, target=example["ideal_response"])
    await feedback.backward()

await optimizer.step()

# Deploy optimized pipeline
module.eval()
```

**Step 3: Continue optimizing with production feedback**

```python
# plait enables continuous improvement
async def handle_request(query: str, user_feedback: str | None = None):
    response = await module(query)

    if user_feedback:
        feedback = Feedback(content=user_feedback)
        module.train()
        await feedback.backward()
        module.eval()

    return response

# Periodic batch optimization
async def optimize_from_feedback():
    if optimizer.has_accumulated_feedback():
        await optimizer.step()
```

## When to Use Each

### Choose NeMo Data Designer when:

- You need to **generate training datasets** for fine-tuning
- You want **schema-based data definition** with constraints
- **Diversity and correlation control** are important
- You're generating **large volumes** of synthetic data
- You need **validation pipelines** for data quality

### Choose plait when:

- You're building **inference pipelines** for production
- You want to **optimize prompts** based on feedback
- You need **real-time processing** with async execution
- **Continuous improvement** during deployment is important
- You want **PyTorch-like patterns** for LLM pipelines

### Use Both Together when:

- You need **high-quality training data** AND **optimized inference**
- You want to **bootstrap** a pipeline with synthetic data
- You're building a **full ML lifecycle** for LLM applications
- **Initial optimization** needs seed data, then **continuous learning** takes over

## Feature Comparison

| Feature | NeMo Data Designer | plait |
|---------|-------------------|-------|
| Primary use | Data generation | Inference optimization |
| Input | Schema definition | User queries |
| Output | Datasets | Inference results |
| Optimization | Data quality | Pipeline performance |
| Execution | Batch-oriented | Async-first |
| Validation | Schema validators | Loss functions |
| Diversity | Built-in strategies | DAG structure |
| Continuous learning | N/A | Native support |
| PyTorch-like API | No | Yes |

## Summary

NeMo Data Designer and plait serve different purposes in the LLM application
lifecycle:

- **NeMo Data Designer** generates the training data you need to build and
  evaluate LLM applications
- **plait** optimizes how you use LLMs to process that data in production

They are complementary tools:

1. Use **Data Designer** to create high-quality training examples
2. Use **plait** to build and optimize inference pipelines
3. Continue improving with **plait's backward pass** based on real feedback

This combination gives you both excellent starting data and continuous
improvement during deployment.
