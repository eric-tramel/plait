# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Integrate Value.ref system with optimization components
  - Loss functions now unwrap Value objects before evaluation
  - `Loss.__call__` unwraps both output and target Value objects
  - `Loss._evaluate_batch` unwraps targets in batch mode
  - `Loss._extract_value_and_record` handles Value, TracedOutput, and raw values
  - `_resolve_input_node` in backward pass handles both NodeRef and ValueRef
  - Tests added for Value integration with VerifierLoss, CompositeLoss
- Add `OptimizationError` exception for parameter optimization failures
  - Raised when optimizer cannot produce valid updates after exhausting retries
  - Includes `parameter_name` and `attempts` attributes for debugging
- Add retry logic for `SFAOptimizer` parameter updates
  - New `max_retries` parameter (default: 3) controls retry attempts on empty/invalid LLM responses
  - Validates that update values are non-empty and meaningful
  - Logs warnings on retry attempts and errors on exhausted retries
  - Raises `OptimizationError` when all retries are exhausted
- Execute graphs with ValueRef and error-as-value semantics
  - `Scheduler._run_task_inner()` implements Value(ERROR) short-circuit behavior
  - `Scheduler._direct_execute()` unwraps Values before calling `forward()`
  - `Scheduler._build_llm_request()` unwraps Values for LLM prompts
  - `has_error_value()` helper to check for Value(ERROR) in args/kwargs
  - `first_error_value()` helper to find first Value(ERROR) in args/kwargs
  - Error Values propagate through graphs without executing dependent tasks
  - User-defined `forward()` methods receive raw payloads, not Value containers
- Refactor tracing to Value-driven capture with ValueRef
  - `Tracer.bind_inputs()` method to wrap inputs as Values with input refs
  - `record_call()` now collects dependencies via `Value.ref` using `collect_refs()`
  - `record_call()` stores `ValueRef` placeholders in GraphNode args/kwargs
  - `record_call()` returns `Value` with ref pointing to the node ID
  - `_collect_output_ids()` now handles both Proxy and Value objects
  - `_capture_output_structure()` now handles both Proxy and Value objects
  - New `Tracer.trace_values()` method for Value-driven tracing flow
  - Dependencies discovered by scanning inputs for `Value.ref`
  - ValueRef placeholders replace Values in stored args/kwargs for execution
  - `ExecutionState._resolve_args()` and `_resolve_kwargs()` now resolve `ValueRef` placeholders
- Add functional API for Value operations
- Lift parameters into Value with stable refs
  - `Parameter.description` is now optional when `requires_grad=False`
  - `Parameter.description` is required when `requires_grad=True` (raises ValueError)
  - `Parameter.value` now accepts `Any` type (string, dict, list, int, float, etc.)
  - `valueify(param)` infers `ValueKind` based on parameter value type
  - Structured values (dict, list, tuple) infer `ValueKind.STRUCTURED`
  - Stable refs use `param:<name>` format for named parameters
  - Stable refs use `param:<id>` format for unnamed parameters
  - Module state version tracked in Value metadata for optimization attribution
- Add `Value` container and `ValueRef` helpers for data flow with provenance
  - `ValueKind` enum: TEXT, FSTRING, RESPONSE, STRUCTURED, INT, FLOAT, ERROR, TOOL_RESULT, BINARY, OTHER
  - `Value` dataclass with kind, payload, ref, and meta fields
  - `Value.__getitem__` and `Value.get` for graph-aware structured access
  - `ValueRef` frozen dataclass as placeholder reference during tracing
  - `valueify()` helper to wrap raw values into Value with type inference
  - `unwrap()` helper to extract payload from Value
  - `collect_refs()` helper to recursively collect refs from nested structures
  - `replace_values_with_refs()` helper to substitute Values with ValueRef placeholders
  - Parameter lifting: `valueify(param)` creates Value with stable `param:<name>` ref
- Add topologically-ordered parameter updates with upstream visibility
  - `Optimizer.capture_record()` method to capture `ForwardRecord` during backward passes
  - `Optimizer._records` list accumulates records for step() graph context
  - `Optimizer.zero_feedback()` now also clears accumulated records
  - `SFAOptimizer.step()` updates parameters in forward topological order
  - Downstream parameters receive context about upstream changes for consistency
  - Parameters at the same topological level are updated in parallel
  - All optimizer prompts use XML tags for content demarcation
  - `_propagate_backward()` accepts optional optimizer to capture records
- Add `TracedOutput` for implicit record flow and batch training API
  - `TracedOutput[T]` dataclass wrapping output value with `ForwardRecord`
  - `module.train()` enables training mode where forward passes return `TracedOutput`
  - `module.eval()` returns raw values (default mode)
  - Training mode propagates to child modules recursively
  - `Loss.batch()` method for concurrent batch evaluation with auto-extraction from `TracedOutput`
  - `Loss._extract_value_and_record()` helper for handling `TracedOutput` wrappers
  - `Feedback.backward_batch()` static method for concurrent backward passes
  - Eliminates manual record management in training pipelines
- Add `Optimizer` abstract base class for parameter updates via LLM
  - Follows torch.optim pattern: initialized with `module.parameters()`
  - Fixed aliases for internal LLMs: `optimizer/aggregator`, `optimizer/updater`, `optimizer/reasoning`
  - `bind()` method to configure resources
  - `zero_feedback()` method to clear parameter feedback buffers (like `zero_grad()`)
  - Abstract async `step()` method for updating parameters
  - Optional `reasoning_model` parameter for backward-pass reasoning LLM
- Add `SFAOptimizer` (Stochastic Feedback Ascent) implementation
  - Makes small, targeted changes based on accumulated feedback
  - `conservatism` hyperparameter (0-1) controls how aggressive updates are
  - Aggregates multiple feedback items using LLM before generating updates
  - Generates improved parameter values using updater LLM with conservatism guidance
- Add backward pass infrastructure for feedback propagation
  - `BackwardContext` dataclass with node execution context and optional reasoning LLM
  - `BackwardResult` dataclass for collecting input and parameter feedback
  - `_propagate_backward()` function for reverse topological graph traversal
  - `_combine_feedback()` helper for aggregating multiple downstream feedback items
- Add `InferenceModule.backward()` async method with default implementation
  - Passes feedback unchanged to all inputs
  - Override for custom backward logic in subclasses
- Add `LLMInference.backward()` implementation
  - Generates feedback for input prompt with context from downstream
  - Generates detailed parameter feedback for learnable system_prompt
  - Includes parameter description, input/output samples, and improvement suggestions
- Add `VerifierLoss` for programmatic output verification
  - Takes a verifier function returning `(passed: bool, message: str)`
  - Returns score 1.0 for pass, 0.0 for fail
  - Configurable success feedback message
- Add `LLMJudge` for freeform LLM-based evaluation
  - Internal LLMInference module with critical reviewer system prompt
  - Optional `criteria` parameter for focused evaluation
  - `bind()` method for resource configuration
  - Returns qualitative feedback without structured score
- Add `CompositeLoss` for weighted multi-objective optimization
  - Combines multiple loss functions with configurable weights
  - Computes weighted average of component scores
  - Simple concatenation or optional LLM aggregator for feedback synthesis
  - `bind()` method propagates to all components
- Add `FeedbackType` enum for classifying feedback sources
  - `HUMAN`: Feedback from human evaluators
  - `LLM_JUDGE`: Feedback from LLM-based evaluation
  - `VERIFIER`: Feedback from programmatic verification
  - `COMPOSITE`: Aggregated feedback from multiple sources
- Add `Feedback` dataclass for representing evaluation results
  - `content`: The feedback text with improvement suggestions
  - `score`: Optional numeric score normalized to 0-1 range
  - `feedback_type`: Classification of the feedback source
  - `metadata`: Optional dictionary for additional evaluation context
  - `backward()`: Async method to propagate feedback through computation graph
  - String representation shows score in brackets when present: `[0.85] Feedback text`
- Add abstract `Loss` base class for loss functions
  - Defines `__call__` interface for evaluating outputs and returning Feedback
  - `_attach_record()` helper to enable `feedback.backward()` propagation
  - Supports optional `target`, `record`, and `context` parameters
- Add `_propagate_backward()` stub for future backward pass implementation
- Add required `Parameter.description` field for self-documenting optimization
  - Parameter now requires a `description` argument explaining what it represents
  - Description is included in repr and used by optimizers to understand the parameter
- Add `ForwardRecord` dataclass for backward pass support
  - Captures execution graph, node inputs/outputs, module references, execution order, and timing
  - Created by `run()` when `record=True` is passed
  - Provides accessor methods: `get_node_input()`, `get_node_output()`, `get_module()`
- Add `record` parameter to `run()` function
  - When `record=False` (default): returns output as before
  - When `record=True`: returns `(output, ForwardRecord)` tuple for backward propagation
  - ExecutionState tracks node inputs and execution order when recording is enabled
- Add `src/plait/optimization/` package with optimization infrastructure
- Project scaffolding and development tooling
- Design documentation in `design_docs/`
- Development plan and task breakdown
- Initial package structure with `py.typed` marker (PEP 561)
- Parameter class for learnable string values that can be optimized via backward passes
- InferenceModule base class with automatic child/parameter registration
- InferenceModule introspection methods: `children()`, `modules()`, `parameters()`, and `named_*` variants
- InferenceModule `forward()` abstract method and `__call__()` delegation
- LLMInference atomic module for LLM API calls
- Module composition integration tests
- Practical examples in `examples/` demonstrating modules, parameters, and LLM pipelines
- Trace context infrastructure for DAG capture using ContextVar
- Proxy class for symbolic tracing
- GraphNode and InferenceGraph data structures for representing traced execution graphs
- Topological ordering method on InferenceGraph for valid execution order
- Graph traversal methods (ancestors, descendants) on InferenceGraph
- Tracer class foundation with node storage and ID generation
- Input node creation in Tracer for capturing graph entry points
- `record_call()` method to Tracer for capturing module invocations
- `trace()` method to Tracer for executing forward() and returning InferenceGraph
- InferenceModule integration with tracing system via `__call__` method
- Tracing integration tests for nested modules, shared inputs, and dict outputs
- Task and TaskResult types for execution state management
- ExecutionState initialization with graph analysis and dependency tracking
- ExecutionState task management methods: `get_next_task()`, `mark_complete()`, `is_complete()`
- ExecutionState failure handling with `mark_failed()` and descendant cancellation
- ExecutionState `requeue()` method for retrying tasks with descendant dropping
- ExecutionState `get_outputs()` method for retrieving final output values
- Scheduler class with concurrency control via semaphore
- Scheduler `execute()` method for running tasks with TaskGroup and dependency management
- `run()` function for tracing and executing modules end-to-end
- Execution integration tests for linear, parallel, and diamond graph patterns
- Proxy data access operations: `__getitem__`, `__iter__`, `keys()`, `values()`, `items()`
- Tracer methods for data access: `record_getitem()`, `record_iter()`, `record_method()`
- Operation classes: `GetItemOp`, `IterOp`, `MethodOp` for representing data access in graphs
- Preserve user-defined output keys when forward() returns a dict, list, or nested structure
- Add `NodeRef` type for type-safe node references in args and kwargs
- Add cycle detection to `InferenceGraph.topological_order()` with clear error message
- Add `task_ready_event` to ExecutionState for event-driven scheduler coordination
- Add `state_dict()` and `load_state_dict()` to InferenceModule for parameter serialization
- Add `visualize_graph()` function for DOT format graph output
- Add `EndpointConfig` dataclass for LLM endpoint configuration
- Add `ResourceConfig` dataclass for managing multiple LLM endpoints
- Add shared `plait.types` module for core types used across packages
  - `LLMRequest`: prompt, system_prompt, temperature, max_tokens, tools, tool_choice, extra_body
  - `LLMResponse`: content, tokens, finish_reason, reasoning, tool_calls, timing metrics
- Add `LLMClient` abstract base class for LLM provider clients
  - Defines async `complete(request: LLMRequest) -> LLMResponse` interface
  - Provides unified contract for OpenAI, Anthropic, vLLM, and other providers
- Add `OpenAIClient` implementation for OpenAI API
  - Async completion with message formatting and tool call support
  - Rate limit handling with `RateLimitError` and retry-after extraction
  - Configurable base URL, API key, and timeout
- Add `OpenAICompatibleClient` for self-hosted models (vLLM, TGI, Ollama)
  - Inherits from `OpenAIClient` with simplified configuration
  - Required `base_url` parameter for custom endpoints
  - Default `api_key="not-needed"` for local deployments
- Add `ResourceManager` initialization for endpoint coordination
  - Creates LLM clients based on provider_api configuration
  - Creates per-endpoint semaphores for concurrency control
  - Supports openai and vllm providers
- Add ResourceManager integration to Scheduler for LLM execution
  - Scheduler accepts optional `resource_manager` parameter
  - LLMInference modules are executed through ResourceManager's clients
  - Per-endpoint semaphores provide concurrency control for LLM calls
  - `_build_llm_request()` helper builds LLMRequest from module config and args
- Add custom error types for error handling and recovery
  - `InfEngineError`: Base exception for all plait errors
  - `RateLimitError`: Raised when API rate limits are hit, with optional `retry_after`
  - `ExecutionError`: Raised on task execution failures, with optional `node_id` and `cause`
- Add `RateLimiter` with token bucket algorithm for endpoint rate control
  - Configurable RPM (requests per minute) and max_tokens (burst capacity)
  - Async `acquire()` method waits for available tokens
  - Thread-safe implementation using asyncio.Lock
- Add adaptive backoff to `RateLimiter` for automatic rate adjustment
  - `backoff()` method reduces rate when hitting API limits
  - `recover()` method gradually restores rate after successful requests
  - Configurable `min_rpm`, `recovery_factor`, and `backoff_factor`
  - Supports server-provided `retry_after` hints for optimal rate adjustment
- Add RateLimiter integration to ResourceManager
  - `rate_limiters` dict holds per-endpoint rate limiters
  - `get_rate_limiter()` method retrieves rate limiter for an alias
  - Rate limiters created automatically when `EndpointConfig.rate_limit` is set
- Add RateLimitError handling to Scheduler
  - Catch `RateLimitError` in `_execute_task` and trigger task requeue
  - Call `backoff()` on the endpoint's rate limiter when rate limits are hit
  - Tasks are automatically retried without being marked as failed
  - `retry_after` value from the error is passed to the rate limiter for optimal backoff
- Add `Checkpoint` dataclass for execution state persistence
  - `execution_id`, `timestamp`, `completed_nodes`, `failed_nodes`, `pending_nodes` attributes
  - `graph_hash` field for detecting incompatible checkpoints when pipeline changes
  - `save(path)` method for JSON serialization to disk
  - `load(path)` classmethod for deserializing from disk
  - `is_compatible_with(hash)` method for verifying checkpoint compatibility
  - Enables progress recovery for long-running pipelines
- Add `InferenceGraph.compute_hash()` for deterministic graph fingerprinting
  - Merkle-tree style hash based on module types, configurations, and dependencies
  - Independent of node IDs - same logical structure produces same hash
  - Enables checkpoint validation across different Python sessions
- Add `CheckpointManager` for buffered checkpoint writes during execution
  - Configurable `buffer_size` and `flush_interval` for efficient disk I/O
  - Per-execution buffers for parallel run tracking
  - `record_completion()` for buffering task results
  - `flush()` and `flush_all()` for persisting buffered completions
  - `get_checkpoint()` for retrieving existing checkpoints
  - `set_graph_hash()` for storing graph hash in checkpoints
- Add `checkpoint_dir` and `execution_id` parameters to `run()` function
  - Enables automatic progress checkpointing during execution
  - Auto-generates UUID execution_id if not provided
  - Creates checkpoint directory if it doesn't exist
  - Stores graph hash for checkpoint compatibility checking
- Add checkpointing example in `examples/06_checkpointing.py`
- Add `ExecutionSettings` context manager for shared execution configuration
  - Dataclass with `resources`, `checkpoint_dir`, `max_concurrent`, `scheduler`, and callback fields
  - Both sync (`with`) and async (`async with`) context manager protocols
  - `get_execution_settings()` function for accessing current settings
  - Nested context support with proper restoration and value inheritance
  - Automatic `CheckpointManager` creation when `checkpoint_dir` is set
  - Profile configuration fields reserved for future profiling integration
- Integrate ExecutionSettings with InferenceModule for direct module execution
  - Add `bind(resources, max_concurrent, **kwargs)` method for binding resources to modules
  - Update `__call__` to check for resources from both bound config and ExecutionSettings context
  - Add `_execute_bound()` async method for tracing and executing bound modules
  - Configuration priority order: call-time kwargs > bound settings > context settings > defaults
  - Add `resources` parameter to `run()` function for passing ResourceConfig/ResourceManager
  - Support batch execution: `await module([input1, input2, ...])` returns list of results
- Concurrent batch execution: batch inputs now run in parallel using `asyncio.gather()` for maximum throughput
- Add `run_sync()` method for synchronous blocking execution in scripts and notebooks
  - Blocks until execution completes and returns the result
  - Supports both single and batch inputs
  - Raises `RuntimeError` if called from async context or without bound resources
- Add streaming execution with BatchResult, progress tracking, and cancellation
  - New `BatchResult` type wraps streaming results with index, input, output, and error fields
  - `BatchResult.ok` property for easy success/failure checking
  - `ExecutionSettings.streaming=True` enables async iteration over batch results
  - `ExecutionSettings.preserve_order` controls result ordering (completion vs input order)
  - `ExecutionSettings.on_progress` callback tracks batch progress (completed, total)
  - `InferenceModule._stream_batch()` method for internal streaming implementation
  - Cancellation support: breaking from streaming loop cancels all pending tasks
  - Progress callbacks work in both streaming and non-streaming batch modes
- Add resource management integration tests (`tests/integration/test_resources.py`)
  - Multiple endpoint alias tests with different configurations
  - Concurrent request handling with semaphore limits
  - ResourceManager and Scheduler integration tests
  - ExecutionSettings with resource context tests
  - Batch execution across multiple endpoints
  - End-to-end pipeline tests with mocked LLM endpoints
  - Configuration example validation from design docs
- Update examples documentation with best practices for resource configuration
- Add `ResourceMetrics` for endpoint observability
  - `EndpointMetrics` dataclass tracks per-endpoint request counts, latency, and tokens
  - Thread-safe `ResourceMetrics` class aggregates metrics across endpoints
  - `record_success()`, `record_error()`, `record_rate_limit()` for recording outcomes
  - `get_alias_stats()` and `get_all_stats()` for retrieving metrics
  - `estimate_cost()` for calculating costs based on token usage and endpoint pricing
  - `reset()` and `get_endpoint_metrics()` for testing and detailed access
- Integrate `ResourceMetrics` with `ResourceManager`
  - `metrics` attribute automatically created during initialization
  - `get_stats()` method returns availability and metrics for all endpoints
- Add task timeout and retry handling with TransientError
  - `TransientError` for retryable failures (connection timeouts, server errors)
  - `ExecutionSettings.task_timeout` sets maximum seconds per task
  - `ExecutionSettings.max_task_retries` controls retry attempts for transient failures
  - `ExecutionSettings.task_retry_delay` sets base delay with exponential backoff
  - Scheduler uses `asyncio.timeout()` for task timeout enforcement
  - Timed-out tasks are marked as failed with descriptive error message
  - TransientError triggers automatic retry with exponential backoff (delay doubles each retry)
  - Getter methods on ExecutionSettings: `get_task_timeout()`, `get_max_task_retries()`, `get_task_retry_delay()`
- Add production features integration tests (`tests/integration/test_*.py`)
  - `test_rate_limiting.py`: Rate limit requeue, backoff, recovery, multiple endpoint handling
  - `test_checkpointing.py`: Checkpoint creation, persistence, compatibility, multiple pipelines
  - `test_reliability.py`: Task timeout, transient error retry, exponential backoff, cancellation cascade
  - `test_execution_settings.py`: Extended with timeout/retry settings, multiple pipeline sharing
- Add profiling infrastructure with TraceProfiler and ExecutionSettings integration
  - `TraceEvent` dataclass representing Chrome Trace Event Format events
  - `TraceProfiler` class for collecting execution traces with task lifecycle methods
  - `ProfilerStatistics` and `EndpointStats` for execution metrics and per-endpoint breakdowns
  - Task lifecycle events: `task_start()`, `task_end()`, `task_failed()` for duration tracking
  - Rate limiting events: `rate_limit_hit()`, `rate_limit_wait_start/end()` for backpressure visualization
  - Queue events: `task_queued()`, `task_dequeued()` for queue depth tracking
  - Counter events for real-time metrics (queue depth, per-endpoint concurrency)
  - Custom instant events via `add_instant_event()` for user-defined markers
  - `export()` method generates Chrome Trace Format JSON for Perfetto/Chrome DevTools
  - `get_statistics()` computes aggregate metrics including completed/failed counts, latency stats
  - Thread-safe implementation for concurrent task profiling
  - Scheduler integration: automatic task start/end/fail profiling when profiler is provided
  - ExecutionSettings integration: `profile`, `profile_path`, `profile_counters`, `profile_include_args` options
  - Automatic profiler creation on context entry and trace export on context exit
  - `profiler` property and `get_profiler()` method for accessing the profiler instance
  - `test_profiling.py` integration tests for trace generation, event correctness, multi-endpoint traces
  - Add profiling example in `examples/08_profiling.py`
- Add additional loss functions for comprehensive evaluation strategies
  - `RubricLevel` dataclass for defining Likert scale rubric levels
  - `RubricResponse`, `PreferenceResponse`, `RankingResponse` structured output schemas
  - `HumanFeedbackLoss` for freeform human feedback collection via stdin
  - `LLMRubricLoss` for LLM evaluation against structured Likert scale rubrics
  - `HumanRubricLoss` for human evaluation against structured Likert scale rubrics
  - `ContrastiveLoss` base class for comparison-based loss functions
  - `LLMPreferenceLoss` for LLM pairwise preference comparison
  - `HumanPreferenceLoss` for human pairwise preference comparison
  - `LLMRankingLoss` for LLM n-way ranking of multiple outputs
  - `HumanRankingLoss` for human n-way ranking of multiple outputs

### Changed
- Replace scheduler busy-wait polling with `asyncio.Event` signaling for efficient task-ready notifications
- Standardize rate limiting units to RPM (requests per minute) across all APIs
  - `RateLimiter` now accepts `rpm` parameter instead of `rate`
  - `EndpointConfig.rate_limit` is now documented as requests per minute
  - Aligns with LLM API provider conventions (OpenAI, Anthropic, etc.)
- Refactor batch loss API to match PyTorch semantics (PR-068c)
  - `Loss.__call__` now auto-detects list inputs and returns single aggregated `Feedback`
  - `Feedback._records` replaces `_record` to hold multiple `ForwardRecord` objects
  - `Feedback.backward()` now propagates to all attached records concurrently
  - Aggregated feedback includes mean score reduction and all records from batch
  - Individual scores available in `feedback.metadata["individual_scores"]`
  - Matches PyTorch pattern: `loss.backward()` on reduced loss propagates to all samples

### Deprecated
- N/A

### Removed
- `Loss.batch()` method - use `loss(outputs_list)` directly (auto-detects batch)
- `Feedback.backward_batch()` method - use `feedback.backward()` on aggregated feedback

### Fixed
- Standardize priority ordering convention to "lower value = higher precedence" across GraphNode and Task

### Security
- N/A

---

## Version History

_No releases yet._

---

## Release Process

1. Update version in `pyproject.toml`
2. Move items from `[Unreleased]` to new version section
3. Add release date
4. Create git tag: `git tag -a v0.X.0 -m "Release v0.X.0"`
5. Push tag: `git push origin v0.X.0`
